{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/asafe-eduardo/colab/blob/master/rnn_from_scratch/lstm_gru_with_tensorflow_eduardo_asafe.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fZUPFV0wgB6p"
   },
   "source": [
    "# OBJECTIVES #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E9tu0uZEgB6r"
   },
   "source": [
    "Para as datas devem predizer qual são os montantes para os próximos pontos de entrada na validação e teste. E verificar em uma outra predição os pontos de melhores clientes. Lembrando que não quero a melhor predição e sim como irá ficar a arquitetura e as explicações."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nB2RCFgNgB6t"
   },
   "source": [
    "### GENERAL INFO\n",
    "\n",
    "`Account No.` - representa o número da conta envolvida na transação.\n",
    "\n",
    "`Date` - data da transação.\n",
    "\n",
    "`Transaction Details` - narrações de transação em extratos bancários.\n",
    "\n",
    "`Cheque No.` - indica o número do cheque.\n",
    "\n",
    "`Value Date` - Data de conclusão da transação.\n",
    "\n",
    "`Withdrawal Amount` - Indica o montante retirado.\n",
    "\n",
    "`Deposit Amount` - Indica o valor depositado.\n",
    "\n",
    "`Balance Amount` - saldo atual da conta.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Vb94M2tHXpI9"
   },
   "source": [
    "### IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BVMZZqQiSTxW"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "braDqTlNgB64",
    "outputId": "7c9087ef-3935-44c2-f920-e310339e8e7a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.14.0\n"
     ]
    }
   ],
   "source": [
    "tf_version = tf.__version__\n",
    "print(tf_version) ## 1.14\n",
    "\n",
    "tf.reset_default_graph() ## reset graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aFW2p4LqgB7A"
   },
   "source": [
    "### LOAD DATA ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M7oKzjKIgB7B"
   },
   "outputs": [],
   "source": [
    "dataframe = pd.read_excel(\"./bank.xlsx\", Header=0, parser_dates=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "colab_type": "code",
    "id": "MDGhOuuQgB7F",
    "outputId": "1017ee19-a45a-4bcd-e2a5-45893e39a46e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Account No</th>\n",
       "      <th>DATE</th>\n",
       "      <th>TRANSACTION DETAILS</th>\n",
       "      <th>CHQ.NO.</th>\n",
       "      <th>VALUE DATE</th>\n",
       "      <th>WITHDRAWAL AMT</th>\n",
       "      <th>DEPOSIT AMT</th>\n",
       "      <th>BALANCE AMT</th>\n",
       "      <th>.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>409000611074'</td>\n",
       "      <td>2017-06-29</td>\n",
       "      <td>TRF FROM  Indiaforensic SERVICES</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-06-29</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1000000.0</td>\n",
       "      <td>1000000.0</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>409000611074'</td>\n",
       "      <td>2017-07-05</td>\n",
       "      <td>TRF FROM  Indiaforensic SERVICES</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-07-05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1000000.0</td>\n",
       "      <td>2000000.0</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>409000611074'</td>\n",
       "      <td>2017-07-18</td>\n",
       "      <td>FDRL/INTERNAL FUND TRANSFE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-07-18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>500000.0</td>\n",
       "      <td>2500000.0</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>409000611074'</td>\n",
       "      <td>2017-08-01</td>\n",
       "      <td>TRF FRM  Indiaforensic SERVICES</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-08-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3000000.0</td>\n",
       "      <td>5500000.0</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>409000611074'</td>\n",
       "      <td>2017-08-16</td>\n",
       "      <td>FDRL/INTERNAL FUND TRANSFE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-08-16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>500000.0</td>\n",
       "      <td>6000000.0</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Account No       DATE               TRANSACTION DETAILS  CHQ.NO.  \\\n",
       "0  409000611074' 2017-06-29  TRF FROM  Indiaforensic SERVICES      NaN   \n",
       "1  409000611074' 2017-07-05  TRF FROM  Indiaforensic SERVICES      NaN   \n",
       "2  409000611074' 2017-07-18        FDRL/INTERNAL FUND TRANSFE      NaN   \n",
       "3  409000611074' 2017-08-01   TRF FRM  Indiaforensic SERVICES      NaN   \n",
       "4  409000611074' 2017-08-16        FDRL/INTERNAL FUND TRANSFE      NaN   \n",
       "\n",
       "  VALUE DATE  WITHDRAWAL AMT  DEPOSIT AMT  BALANCE AMT  .  \n",
       "0 2017-06-29             NaN    1000000.0    1000000.0  .  \n",
       "1 2017-07-05             NaN    1000000.0    2000000.0  .  \n",
       "2 2017-07-18             NaN     500000.0    2500000.0  .  \n",
       "3 2017-08-01             NaN    3000000.0    5500000.0  .  \n",
       "4 2017-08-16             NaN     500000.0    6000000.0  .  "
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe.head() ## a taste of dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "colab_type": "code",
    "id": "mFifn6FJgB7K",
    "outputId": "726eb2f4-25a8-412a-8eb0-48ea442ad455",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 116201 entries, 0 to 116200\n",
      "Data columns (total 9 columns):\n",
      "Account No             116201 non-null object\n",
      "DATE                   116201 non-null datetime64[ns]\n",
      "TRANSACTION DETAILS    113702 non-null object\n",
      "CHQ.NO.                905 non-null float64\n",
      "VALUE DATE             116201 non-null datetime64[ns]\n",
      "WITHDRAWAL AMT         53549 non-null float64\n",
      "DEPOSIT AMT            62652 non-null float64\n",
      "BALANCE AMT            116201 non-null float64\n",
      ".                      116201 non-null object\n",
      "dtypes: datetime64[ns](2), float64(4), object(3)\n",
      "memory usage: 8.0+ MB\n"
     ]
    }
   ],
   "source": [
    "dataframe.info() ##print infos of dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RIj1b-BQgB7O"
   },
   "source": [
    "Exist 9 columns.\n",
    "\n",
    "total de rows = 116201\n",
    "\n",
    "the columns `TRANSACTION DETAILS, CHQ.NO., WITHDRAWAL AMT, DEPOSIT AMT` have less rows that dataframe has so we need did do the following strategy:\n",
    "    - cut off the columns with few values\n",
    "    - fill the empty columns values with some value\n",
    "    \n",
    "but before this let's taste a flavor of values of the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "LSUu7Xc9gB7P",
    "outputId": "8260bf6b-fe03-4426-c145-45f68f9eb27b",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================   PRINT VALUES ========================================\n",
      "column: Account No, \n",
      "values: [u\"409000611074'\" u\"409000493201'\" u\"409000425051'\" u\"409000405747'\"\n",
      " u\"409000438611'\" u\"409000493210'\" u\"409000438620'\" u\"1196711'\"\n",
      " u\"1196428'\" u\"409000362497'\"]\n",
      "============================= COLUMN INFO ======================================\n",
      "unique values in column: Account No = 10\n",
      "\n",
      "\n",
      "\n",
      "========================   PRINT VALUES ========================================\n",
      "column: DATE, \n",
      "values: ['2017-06-29T00:00:00.000000000' '2017-07-05T00:00:00.000000000'\n",
      " '2017-07-18T00:00:00.000000000' '2017-08-01T00:00:00.000000000'\n",
      " '2017-08-16T00:00:00.000000000' '2017-09-06T00:00:00.000000000'\n",
      " '2017-09-25T00:00:00.000000000' '2017-09-26T00:00:00.000000000'\n",
      " '2017-09-27T00:00:00.000000000' '2017-09-28T00:00:00.000000000'\n",
      " '2017-10-03T00:00:00.000000000' '2017-10-04T00:00:00.000000000'\n",
      " '2017-10-05T00:00:00.000000000' '2017-10-06T00:00:00.000000000'\n",
      " '2017-10-16T00:00:00.000000000' '2017-10-17T00:00:00.000000000'\n",
      " '2017-10-18T00:00:00.000000000' '2017-10-21T00:00:00.000000000'\n",
      " '2017-10-23T00:00:00.000000000' '2017-10-24T00:00:00.000000000']\n",
      "============================= COLUMN INFO ======================================\n",
      "unique values in column: DATE = 1294\n",
      "\n",
      "\n",
      "\n",
      "========================   PRINT VALUES ========================================\n",
      "column: TRANSACTION DETAILS, \n",
      "values: [u'TRF FROM  Indiaforensic SERVICES' u'FDRL/INTERNAL FUND TRANSFE'\n",
      " u'TRF FRM  Indiaforensic SERVICES' u'INDO GIBL Indiaforensic STL01071'\n",
      " u'INDO GIBL Indiaforensic STL02071' u'INDO GIBL Indiaforensic STL03071'\n",
      " u'INDO GIBL Indiaforensic STL04071' u'INDO GIBL Indiaforensic STL05071'\n",
      " u'INDO GIBL Indiaforensic STL06071' u'INDO GIBL Indiaforensic STL07071'\n",
      " u'INDO GIBL Indiaforensic STL10071' u'INDO GIBL Indiaforensic STL11071'\n",
      " u'INDO GIBL Indiaforensic STL12071' u'INDO GIBL Indiaforensic STL13071'\n",
      " u'INDO GIBL Indiaforensic STL14071' u'INDO GIBL Indiaforensic STL15071'\n",
      " u'INDO GIBL Indiaforensic STL16071' u'INDO GIBL Indiaforensic STL17071'\n",
      " u'INDO GIBL Indiaforensic STL18071' u'INDO GIBL Indiaforensic STL19071']\n",
      "============================= COLUMN INFO ======================================\n",
      "unique values in column: TRANSACTION DETAILS = 44807\n",
      "\n",
      "\n",
      "\n",
      "========================   PRINT VALUES ========================================\n",
      "column: CHQ.NO., \n",
      "values: [        nan 1.00000e+00 2.00000e+00 5.00000e+00 6.00000e+00 4.00000e+00\n",
      " 3.00000e+00 9.00000e+00 7.00000e+00 8.00000e+00 1.00000e+01 8.73777e+05\n",
      " 8.73776e+05 8.73779e+05 8.73780e+05 8.73781e+05 8.73778e+05 8.73733e+05\n",
      " 8.73728e+05 8.73727e+05]\n",
      "============================= COLUMN INFO ======================================\n",
      "unique values in column: CHQ.NO. = 895\n",
      "\n",
      "\n",
      "\n",
      "========================   PRINT VALUES ========================================\n",
      "column: VALUE DATE, \n",
      "values: ['2017-06-29T00:00:00.000000000' '2017-07-05T00:00:00.000000000'\n",
      " '2017-07-18T00:00:00.000000000' '2017-08-01T00:00:00.000000000'\n",
      " '2017-08-16T00:00:00.000000000' '2017-09-06T00:00:00.000000000'\n",
      " '2017-09-25T00:00:00.000000000' '2017-09-26T00:00:00.000000000'\n",
      " '2017-09-27T00:00:00.000000000' '2017-09-28T00:00:00.000000000'\n",
      " '2017-10-03T00:00:00.000000000' '2017-10-04T00:00:00.000000000'\n",
      " '2017-10-05T00:00:00.000000000' '2017-10-06T00:00:00.000000000'\n",
      " '2017-10-16T00:00:00.000000000' '2017-10-17T00:00:00.000000000'\n",
      " '2017-10-18T00:00:00.000000000' '2017-10-21T00:00:00.000000000'\n",
      " '2017-10-23T00:00:00.000000000' '2017-10-24T00:00:00.000000000']\n",
      "============================= COLUMN INFO ======================================\n",
      "unique values in column: VALUE DATE = 1294\n",
      "\n",
      "\n",
      "\n",
      "========================   PRINT VALUES ========================================\n",
      "column: WITHDRAWAL AMT, \n",
      "values: [    nan 133900.  18000.   5000. 195800.  81600.  41800.  98500. 143800.\n",
      " 331650. 129000. 230013. 367900. 108000.  64800. 141000.  61750.  67920.\n",
      "  78100.  35650.]\n",
      "============================= COLUMN INFO ======================================\n",
      "unique values in column: WITHDRAWAL AMT = 20882\n",
      "\n",
      "\n",
      "\n",
      "========================   PRINT VALUES ========================================\n",
      "column: DEPOSIT AMT, \n",
      "values: [1.000000e+06 5.000000e+05 3.000000e+06          nan 1.320000e+03\n",
      " 1.555000e+05 2.027990e+05 2.000000e+05 3.000000e+05 4.000000e+05\n",
      " 9.000000e-01 6.700000e+04 1.170000e+05 1.500000e+05 1.000000e+05\n",
      " 5.000000e+04 6.654900e+02 4.721693e+04 7.500000e+05 1.671630e+03]\n",
      "============================= COLUMN INFO ======================================\n",
      "unique values in column: DEPOSIT AMT = 20146\n",
      "\n",
      "\n",
      "\n",
      "========================   PRINT VALUES ========================================\n",
      "column: BALANCE AMT, \n",
      "values: [1000000. 2000000. 2500000. 5500000. 6000000. 6500000. 7000000. 7500000.\n",
      " 8000000. 8500000. 8366100. 8348100. 8343100. 8147300. 8065700. 8023900.\n",
      " 7925400. 7781600. 7449950. 7320950.]\n",
      "============================= COLUMN INFO ======================================\n",
      "unique values in column: BALANCE AMT = 115171\n",
      "\n",
      "\n",
      "\n",
      "========================   PRINT VALUES ========================================\n",
      "column: ., \n",
      "values: [u'.']\n",
      "============================= COLUMN INFO ======================================\n",
      "unique values in column: . = 1\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for column in dataframe.columns:\n",
    "    print('========================   PRINT VALUES ========================================')\n",
    "    print('column: {0}, \\nvalues: {1}'.format(column, dataframe[column].unique()[0:20]))\n",
    "    print('============================= COLUMN INFO ======================================')\n",
    "    print('unique values in column: {0} = {1}'.format(column, len(dataframe[column].unique())))\n",
    "    print('\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rSdvbY6pgB7U"
   },
   "source": [
    "what we discovered from values of the dataframe?\n",
    "    - `Account No` have just 10 differents values the means the dataframe has just 10 account\n",
    "    - `TRANSACTION DETAILS` have more of 40.000 values that means very diferents description to transactions\n",
    "    - for our luck all dates have values \\o/ \n",
    "    \n",
    "we discovered a strange column `.` that has a unique value '.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v3lwvxFnfE7i"
   },
   "source": [
    "### PREPROCESSING ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EyqG50N_gB7X"
   },
   "source": [
    "now we start preprocessing our dataframe, in this process we will drop and fill values that is empty to some value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HjKcMnc6gB7Z"
   },
   "outputs": [],
   "source": [
    "dataframe.drop(columns=['TRANSACTION DETAILS', 'CHQ.NO.', '.'], inplace=True); ##drop columns that we not will use\n",
    "for column in dataframe.columns:\n",
    "    if(dataframe[column].isnull().any()):\n",
    "        dataframe[column].fillna(value=dataframe[column].median(), inplace=True) ## filling our nan values with median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "wR3jJLKagB7h",
    "outputId": "a0396efd-8b47-4970-fc60-cc6c254d858c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 116201 entries, 0 to 116200\n",
      "Data columns (total 6 columns):\n",
      "Account No        116201 non-null object\n",
      "DATE              116201 non-null datetime64[ns]\n",
      "VALUE DATE        116201 non-null datetime64[ns]\n",
      "WITHDRAWAL AMT    116201 non-null float64\n",
      "DEPOSIT AMT       116201 non-null float64\n",
      "BALANCE AMT       116201 non-null float64\n",
      "dtypes: datetime64[ns](2), float64(3), object(1)\n",
      "memory usage: 5.3+ MB\n"
     ]
    }
   ],
   "source": [
    "dataframe.info() ### printing now our values processed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UP6WtKY_gB7m"
   },
   "source": [
    "now how you can see our dataframe has all rows filled.\n",
    "we removed the columns that have fews values and not means nothing for our objectives.\n",
    "we filled the empty values with median that is the 'the median value of a range', we keep the `Account No` column because we need predict what client is a good client.\n",
    "\n",
    "Our Preprocessing its not finished yet, we need now transform the `Account No` and do a couple of things. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hpmkP1gngB7n"
   },
   "outputs": [],
   "source": [
    "enc = OrdinalEncoder()\n",
    "\n",
    "def categorical_to_ordinal(categorical_values, serie): ## method to convert categorical string to categorical integer\n",
    "    enc.fit(categorical_values)\n",
    "    serie_reshape = np.reshape(serie.to_numpy(), (-1, 1))\n",
    "    return enc.transform(serie_reshape).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c6HBL7PdgB7r"
   },
   "outputs": [],
   "source": [
    "accounts_label = dataframe['Account No'].unique()\n",
    "accounts = dataframe['Account No']\n",
    "#accounts_ordinal = categoricalToOrdinal([[u\"409000611074'\"],[u\"409000493201'\"], [u\"409000425051'\"],[u\"409000405747'\"], [u\"409000438611'\"], [u\"409000493210'\"],[u\"409000438620'\"], [u\"1196711'\"], [u\"1196428'\"], [u\"409000362497'\"]], accounts);\n",
    "accounts_ordinal = categorical_to_ordinal(accounts_label.reshape(-1,1), accounts);\n",
    "dataframe['client_class'] = pd.Series(accounts_ordinal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GflAM1xFgB7u"
   },
   "outputs": [],
   "source": [
    "epoch = datetime.datetime.utcfromtimestamp(0)\n",
    "\n",
    "def unix_time_millis(dt): ## method to convert timestamp in milliseconds\n",
    "    return (dt - epoch).total_seconds() * 1000.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3v1YhwYMgB7x"
   },
   "outputs": [],
   "source": [
    "dataframe['DATE'] = dataframe['DATE'].apply(unix_time_millis)\n",
    "dataframe['VALUE DATE'] = dataframe['VALUE DATE'].apply(unix_time_millis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YAj4Z-0BgB70"
   },
   "outputs": [],
   "source": [
    "def min_max_scaler(data): ## method to scaler our values\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(data.to_numpy().reshape(-1, 1))\n",
    "    transformed_data = scaler.transform(data.to_numpy().reshape(-1, 1))\n",
    "    return transformed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9oByOZy6gB74",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataframe['DATE'] = min_max_scaler(dataframe['DATE'])\n",
    "dataframe['VALUE DATE'] = min_max_scaler(dataframe['VALUE DATE'])\n",
    "dataframe['WITHDRAWAL AMT'] = min_max_scaler(dataframe['WITHDRAWAL AMT'])\n",
    "dataframe['WITHDRAWAL AMT'] = min_max_scaler(dataframe['WITHDRAWAL AMT'])\n",
    "dataframe['DEPOSIT AMT'] = min_max_scaler(dataframe['DEPOSIT AMT'])\n",
    "dataframe['BALANCE AMT'] = min_max_scaler(dataframe['BALANCE AMT']); ## OMIT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "O-Si-6UTgB77"
   },
   "source": [
    "we create 3 functions to transform our data:\n",
    "    - `categorical_to_ordinal` we need transform the string value of `Account No` to a categorical numeric value and this method give this power to us.\n",
    "    - `unix_time_millis`  a method to convert our timestamp to milliseconds(use just if our NN don't recognize timestamp value).\n",
    "    - `min_max_scaler` different range of values maybe bias our model (grandient not converge to global minimum), also raise model perfomance.\n",
    "    \n",
    "now lets see how our dataframe stayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_BnT4csTgB7_"
   },
   "outputs": [],
   "source": [
    "dataframe.drop(columns=['Account No'], inplace=True); ##drop unnecessary column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "X2Nv21NjgB8K",
    "outputId": "3f626209-3ff6-47d4-b70a-6f2fdea51105",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DATE</th>\n",
       "      <th>VALUE DATE</th>\n",
       "      <th>WITHDRAWAL AMT</th>\n",
       "      <th>DEPOSIT AMT</th>\n",
       "      <th>BALANCE AMT</th>\n",
       "      <th>client_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.597113</td>\n",
       "      <td>0.597113</td>\n",
       "      <td>0.000102</td>\n",
       "      <td>0.001836</td>\n",
       "      <td>0.996348</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.601050</td>\n",
       "      <td>0.601050</td>\n",
       "      <td>0.000102</td>\n",
       "      <td>0.001836</td>\n",
       "      <td>0.996835</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.609580</td>\n",
       "      <td>0.609580</td>\n",
       "      <td>0.000102</td>\n",
       "      <td>0.000918</td>\n",
       "      <td>0.997078</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.618766</td>\n",
       "      <td>0.618766</td>\n",
       "      <td>0.000102</td>\n",
       "      <td>0.005507</td>\n",
       "      <td>0.998539</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.628609</td>\n",
       "      <td>0.628609</td>\n",
       "      <td>0.000102</td>\n",
       "      <td>0.000918</td>\n",
       "      <td>0.998783</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       DATE  VALUE DATE  WITHDRAWAL AMT  DEPOSIT AMT  BALANCE AMT  \\\n",
       "0  0.597113    0.597113        0.000102     0.001836     0.996348   \n",
       "1  0.601050    0.601050        0.000102     0.001836     0.996835   \n",
       "2  0.609580    0.609580        0.000102     0.000918     0.997078   \n",
       "3  0.618766    0.618766        0.000102     0.005507     0.998539   \n",
       "4  0.628609    0.628609        0.000102     0.000918     0.998783   \n",
       "\n",
       "   client_class  \n",
       "0           9.0  \n",
       "1           9.0  \n",
       "2           9.0  \n",
       "3           9.0  \n",
       "4           9.0  "
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MQQCVGJugB8U"
   },
   "source": [
    "now is time to split our data \\o/\n",
    "our label will be `client_class`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xLDUhz9DgB8V"
   },
   "outputs": [],
   "source": [
    "y = dataframe.pop('WITHDRAWAL AMT') ##get client_class\n",
    "X = dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xtEVImhggB8Z"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) ## get test data\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42) ## get val data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "T1f22K-TgB8c"
   },
   "outputs": [],
   "source": [
    "y_train = y_train.to_numpy()\n",
    "y_val = y_val.to_numpy()\n",
    "y_test = y_test.to_numpy()\n",
    "\n",
    "y_train = y_train.reshape(-1, 1)\n",
    "y_val = y_val.reshape(-1, 1)\n",
    "y_test = y_test.reshape(-1, 1)\n",
    "\n",
    "X_train = X_train.to_numpy().reshape(-1, 5, 1)\n",
    "X_val = X_val.to_numpy().reshape(-1, 5, 1)\n",
    "X_test = X_test.to_numpy().reshape(-1, 5, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tAmvHK97gB8f"
   },
   "source": [
    "preprocessing done! :D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J3hkS12lgB8g"
   },
   "source": [
    "### MODEL ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bzdc79fugB8h"
   },
   "source": [
    "now with our data finish let's start building our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0_KepSvPgB8i"
   },
   "outputs": [],
   "source": [
    "## initializing our hyperparams\n",
    "n_inputs = 5\n",
    "n_neurons = 200\n",
    "n_outputs = 1\n",
    "n_layers = 2\n",
    "learning_rate = 0.00001\n",
    "batch_size = 128\n",
    "n_epochs = 500\n",
    "train_set_size = X_train.shape[0]\n",
    "test_set_size = X_test.shape[0]\n",
    "\n",
    "input_X = tf.placeholder(tf.float64, [None, n_inputs, 1])\n",
    "input_y = tf.placeholder(tf.float64, [None, n_outputs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zVA5iVztgB8k"
   },
   "outputs": [],
   "source": [
    "index_in_epoch = 0;\n",
    "perm_array = np.arange(X_train.shape[0])\n",
    "np.random.shuffle(perm_array)\n",
    "\n",
    "def get_next_batch(batch_size): ## function to batch our data\n",
    "    global index_in_epoch, X_train, perm_array\n",
    "    start = index_in_epoch\n",
    "    index_in_epoch += batch_size\n",
    "    if index_in_epoch > X_train.shape[0]:\n",
    "        np.random.shuffle(perm_array)\n",
    "        start = 0\n",
    "        index_in_epoch = batch_size\n",
    "    end = index_in_epoch\n",
    "    return X_train[perm_array[start:end]], y_train[perm_array[start:end]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tVjuasWxgB8p"
   },
   "outputs": [],
   "source": [
    "layers = [tf.contrib.rnn.GRUCell(num_units=n_neurons, activation=tf.nn.relu) ## with my tests, ReLU fit better\n",
    "          for layer in range(n_layers)] # 2 layer 200 neurons\n",
    "\n",
    "multi_layer = tf.contrib.rnn.MultiRNNCell(layers)\n",
    "rnn_outputs, states = tf.nn.dynamic_rnn(multi_layer, input_X, dtype=tf.float64)\n",
    "stacked_rnn_outputs = tf.reshape(rnn_outputs, [-1, n_neurons])\n",
    "stacked_outputs = tf.layers.dense(stacked_rnn_outputs, n_outputs)\n",
    "outputs = tf.reshape(stacked_outputs, [-1, n_inputs, n_outputs])\n",
    "outputs = outputs[:, n_inputs-1, :] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-x2PZg7HgB8s"
   },
   "outputs": [],
   "source": [
    "loss = tf.reduce_mean(tf.square(outputs - input_y)) # MSE\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss) # GRADIENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "zybJVrcPgB8x",
    "outputId": "9af9f5ea-9cdf-4086-b528-0a3ef8585ff1",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train loss = 0.0005978058 valid loss = 0.0006384102\n",
      "Epoch 2: train loss = 0.0005832272 valid loss = 0.0006237945\n",
      "Epoch 3: train loss = 0.0005690166 valid loss = 0.0006095469\n",
      "Epoch 4: train loss = 0.0005551139 valid loss = 0.0005956084\n",
      "Epoch 5: train loss = 0.0005416693 valid loss = 0.0005821274\n",
      "Epoch 6: train loss = 0.0005285996 valid loss = 0.0005690203\n",
      "Epoch 7: train loss = 0.0005158067 valid loss = 0.0005561891\n",
      "Epoch 8: train loss = 0.0005034269 valid loss = 0.0005437708\n",
      "Epoch 9: train loss = 0.0004914435 valid loss = 0.0005317482\n",
      "Epoch 10: train loss = 0.0004797989 valid loss = 0.0005200637\n",
      "Epoch 11: train loss = 0.0004685275 valid loss = 0.0005087520\n",
      "Epoch 12: train loss = 0.0004577124 valid loss = 0.0004978970\n",
      "Epoch 13: train loss = 0.0004473393 valid loss = 0.0004874844\n",
      "Epoch 14: train loss = 0.0004373710 valid loss = 0.0004774762\n",
      "Epoch 15: train loss = 0.0004278091 valid loss = 0.0004678739\n",
      "Epoch 16: train loss = 0.0004187158 valid loss = 0.0004587407\n",
      "Epoch 17: train loss = 0.0004099298 valid loss = 0.0004499141\n",
      "Epoch 18: train loss = 0.0004015455 valid loss = 0.0004414892\n",
      "Epoch 19: train loss = 0.0003935355 valid loss = 0.0004334393\n",
      "Epoch 20: train loss = 0.0003858768 valid loss = 0.0004257415\n",
      "Epoch 21: train loss = 0.0003783047 valid loss = 0.0004181299\n",
      "Epoch 22: train loss = 0.0003711418 valid loss = 0.0004109278\n",
      "Epoch 23: train loss = 0.0003644268 valid loss = 0.0004041740\n",
      "Epoch 24: train loss = 0.0003579969 valid loss = 0.0003977052\n",
      "Epoch 25: train loss = 0.0003519997 valid loss = 0.0003916702\n",
      "Epoch 26: train loss = 0.0003463578 valid loss = 0.0003859911\n",
      "Epoch 27: train loss = 0.0003409348 valid loss = 0.0003805305\n",
      "Epoch 28: train loss = 0.0003358681 valid loss = 0.0003754267\n",
      "Epoch 29: train loss = 0.0003311912 valid loss = 0.0003707133\n",
      "Epoch 30: train loss = 0.0003267445 valid loss = 0.0003662300\n",
      "Epoch 31: train loss = 0.0003226085 valid loss = 0.0003620580\n",
      "Epoch 32: train loss = 0.0003187451 valid loss = 0.0003581589\n",
      "Epoch 33: train loss = 0.0003151683 valid loss = 0.0003545469\n",
      "Epoch 34: train loss = 0.0003118021 valid loss = 0.0003511453\n",
      "Epoch 35: train loss = 0.0003086576 valid loss = 0.0003479659\n",
      "Epoch 36: train loss = 0.0003057482 valid loss = 0.0003450225\n",
      "Epoch 37: train loss = 0.0003030951 valid loss = 0.0003423363\n",
      "Epoch 38: train loss = 0.0003006377 valid loss = 0.0003398463\n",
      "Epoch 39: train loss = 0.0002983601 valid loss = 0.0003375359\n",
      "Epoch 40: train loss = 0.0002962393 valid loss = 0.0003353821\n",
      "Epoch 41: train loss = 0.0002942519 valid loss = 0.0003333617\n",
      "Epoch 42: train loss = 0.0002924419 valid loss = 0.0003315198\n",
      "Epoch 43: train loss = 0.0002907816 valid loss = 0.0003298282\n",
      "Epoch 44: train loss = 0.0002893020 valid loss = 0.0003283187\n",
      "Epoch 45: train loss = 0.0002879434 valid loss = 0.0003269306\n",
      "Epoch 46: train loss = 0.0002867186 valid loss = 0.0003256774\n",
      "Epoch 47: train loss = 0.0002856263 valid loss = 0.0003245576\n",
      "Epoch 48: train loss = 0.0002846324 valid loss = 0.0003235369\n",
      "Epoch 49: train loss = 0.0002837510 valid loss = 0.0003226300\n",
      "Epoch 50: train loss = 0.0002829771 valid loss = 0.0003218321\n",
      "Epoch 51: train loss = 0.0002822997 valid loss = 0.0003211319\n",
      "Epoch 52: train loss = 0.0002817061 valid loss = 0.0003205168\n",
      "Epoch 53: train loss = 0.0002811948 valid loss = 0.0003199856\n",
      "Epoch 54: train loss = 0.0002807536 valid loss = 0.0003195261\n",
      "Epoch 55: train loss = 0.0002803521 valid loss = 0.0003191059\n",
      "Epoch 56: train loss = 0.0002799944 valid loss = 0.0003187304\n",
      "Epoch 57: train loss = 0.0002796839 valid loss = 0.0003184034\n",
      "Epoch 58: train loss = 0.0002794196 valid loss = 0.0003181234\n",
      "Epoch 59: train loss = 0.0002791998 valid loss = 0.0003178894\n",
      "Epoch 60: train loss = 0.0002790210 valid loss = 0.0003176987\n",
      "Epoch 61: train loss = 0.0002788708 valid loss = 0.0003175382\n",
      "Epoch 62: train loss = 0.0002787440 valid loss = 0.0003174019\n",
      "Epoch 63: train loss = 0.0002786407 valid loss = 0.0003172912\n",
      "Epoch 64: train loss = 0.0002785562 valid loss = 0.0003172008\n",
      "Epoch 65: train loss = 0.0002784769 valid loss = 0.0003171158\n",
      "Epoch 66: train loss = 0.0002784061 valid loss = 0.0003170406\n",
      "Epoch 67: train loss = 0.0002783396 valid loss = 0.0003169692\n",
      "Epoch 68: train loss = 0.0002782793 valid loss = 0.0003169045\n",
      "Epoch 69: train loss = 0.0002782254 valid loss = 0.0003168472\n",
      "Epoch 70: train loss = 0.0002781781 valid loss = 0.0003167985\n",
      "Epoch 71: train loss = 0.0002781333 valid loss = 0.0003167522\n",
      "Epoch 72: train loss = 0.0002780907 valid loss = 0.0003167083\n",
      "Epoch 73: train loss = 0.0002780485 valid loss = 0.0003166642\n",
      "Epoch 74: train loss = 0.0002780063 valid loss = 0.0003166197\n",
      "Epoch 75: train loss = 0.0002779658 valid loss = 0.0003165774\n",
      "Epoch 76: train loss = 0.0002779250 valid loss = 0.0003165346\n",
      "Epoch 77: train loss = 0.0002778854 valid loss = 0.0003164941\n",
      "Epoch 78: train loss = 0.0002778460 valid loss = 0.0003164553\n",
      "Epoch 79: train loss = 0.0002778068 valid loss = 0.0003164178\n",
      "Epoch 80: train loss = 0.0002777724 valid loss = 0.0003163845\n",
      "Epoch 81: train loss = 0.0002777391 valid loss = 0.0003163493\n",
      "Epoch 82: train loss = 0.0002777067 valid loss = 0.0003163156\n",
      "Epoch 83: train loss = 0.0002776754 valid loss = 0.0003162829\n",
      "Epoch 84: train loss = 0.0002776422 valid loss = 0.0003162497\n",
      "Epoch 85: train loss = 0.0002776126 valid loss = 0.0003162194\n",
      "Epoch 86: train loss = 0.0002775845 valid loss = 0.0003161906\n",
      "Epoch 87: train loss = 0.0002775689 valid loss = 0.0003161692\n",
      "Epoch 88: train loss = 0.0002775559 valid loss = 0.0003161504\n",
      "Epoch 89: train loss = 0.0002775427 valid loss = 0.0003161332\n",
      "Epoch 90: train loss = 0.0002775347 valid loss = 0.0003161208\n",
      "Epoch 91: train loss = 0.0002775238 valid loss = 0.0003161067\n",
      "Epoch 92: train loss = 0.0002775081 valid loss = 0.0003160893\n",
      "Epoch 93: train loss = 0.0002774949 valid loss = 0.0003160735\n",
      "Epoch 94: train loss = 0.0002774786 valid loss = 0.0003160551\n",
      "Epoch 95: train loss = 0.0002774655 valid loss = 0.0003160394\n",
      "Epoch 96: train loss = 0.0002774503 valid loss = 0.0003160225\n",
      "Epoch 97: train loss = 0.0002774222 valid loss = 0.0003159952\n",
      "Epoch 98: train loss = 0.0002773928 valid loss = 0.0003159667\n",
      "Epoch 99: train loss = 0.0002773663 valid loss = 0.0003159406\n",
      "Epoch 100: train loss = 0.0002773409 valid loss = 0.0003159149\n",
      "Epoch 101: train loss = 0.0002773043 valid loss = 0.0003158797\n",
      "Epoch 102: train loss = 0.0002772643 valid loss = 0.0003158417\n",
      "Epoch 103: train loss = 0.0002772218 valid loss = 0.0003158018\n",
      "Epoch 104: train loss = 0.0002771820 valid loss = 0.0003157639\n",
      "Epoch 105: train loss = 0.0002771363 valid loss = 0.0003157212\n",
      "Epoch 106: train loss = 0.0002770901 valid loss = 0.0003156781\n",
      "Epoch 107: train loss = 0.0002770408 valid loss = 0.0003156332\n",
      "Epoch 108: train loss = 0.0002769942 valid loss = 0.0003155908\n",
      "Epoch 109: train loss = 0.0002769561 valid loss = 0.0003155549\n",
      "Epoch 110: train loss = 0.0002769178 valid loss = 0.0003155187\n",
      "Epoch 111: train loss = 0.0002768768 valid loss = 0.0003154804\n",
      "Epoch 112: train loss = 0.0002768374 valid loss = 0.0003154436\n",
      "Epoch 113: train loss = 0.0002767960 valid loss = 0.0003154056\n",
      "Epoch 114: train loss = 0.0002767500 valid loss = 0.0003153645\n",
      "Epoch 115: train loss = 0.0002767044 valid loss = 0.0003153245\n",
      "Epoch 116: train loss = 0.0002766667 valid loss = 0.0003152899\n",
      "Epoch 117: train loss = 0.0002766363 valid loss = 0.0003152610\n",
      "Epoch 118: train loss = 0.0002766041 valid loss = 0.0003152305\n",
      "Epoch 119: train loss = 0.0002765692 valid loss = 0.0003151978\n",
      "Epoch 120: train loss = 0.0002765366 valid loss = 0.0003151673\n",
      "Epoch 121: train loss = 0.0002764977 valid loss = 0.0003151325\n",
      "Epoch 122: train loss = 0.0002764581 valid loss = 0.0003150979\n",
      "Epoch 123: train loss = 0.0002764208 valid loss = 0.0003150663\n",
      "Epoch 124: train loss = 0.0002763852 valid loss = 0.0003150368\n",
      "Epoch 125: train loss = 0.0002763523 valid loss = 0.0003150092\n",
      "Epoch 126: train loss = 0.0002763187 valid loss = 0.0003149822\n",
      "Epoch 127: train loss = 0.0002762897 valid loss = 0.0003149599\n",
      "Epoch 128: train loss = 0.0002762637 valid loss = 0.0003149418\n",
      "Epoch 129: train loss = 0.0002762422 valid loss = 0.0003149285\n",
      "Epoch 130: train loss = 0.0002762260 valid loss = 0.0003149194\n",
      "Epoch 131: train loss = 0.0002762143 valid loss = 0.0003149160\n",
      "Epoch 132: train loss = 0.0002762105 valid loss = 0.0003149218\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 133: train loss = 0.0002762112 valid loss = 0.0003149319\n",
      "Epoch 134: train loss = 0.0002762129 valid loss = 0.0003149417\n",
      "Epoch 135: train loss = 0.0002762221 valid loss = 0.0003149599\n",
      "Epoch 136: train loss = 0.0002762250 valid loss = 0.0003149705\n",
      "Epoch 137: train loss = 0.0002762080 valid loss = 0.0003149599\n",
      "Epoch 138: train loss = 0.0002761958 valid loss = 0.0003149545\n",
      "Epoch 139: train loss = 0.0002761878 valid loss = 0.0003149535\n",
      "Epoch 140: train loss = 0.0002761743 valid loss = 0.0003149457\n",
      "Epoch 141: train loss = 0.0002761538 valid loss = 0.0003149292\n",
      "Epoch 142: train loss = 0.0002761229 valid loss = 0.0003149015\n",
      "Epoch 143: train loss = 0.0002761008 valid loss = 0.0003148837\n",
      "Epoch 144: train loss = 0.0002760704 valid loss = 0.0003148557\n",
      "Epoch 145: train loss = 0.0002760238 valid loss = 0.0003148100\n",
      "Epoch 146: train loss = 0.0002759804 valid loss = 0.0003147677\n",
      "Epoch 147: train loss = 0.0002759305 valid loss = 0.0003147173\n",
      "Epoch 148: train loss = 0.0002758844 valid loss = 0.0003146702\n",
      "Epoch 149: train loss = 0.0002758350 valid loss = 0.0003146188\n",
      "Epoch 150: train loss = 0.0002757888 valid loss = 0.0003145707\n",
      "Epoch 151: train loss = 0.0002757428 valid loss = 0.0003145221\n",
      "Epoch 152: train loss = 0.0002757016 valid loss = 0.0003144788\n",
      "Epoch 153: train loss = 0.0002756660 valid loss = 0.0003144428\n",
      "Epoch 154: train loss = 0.0002756118 valid loss = 0.0003143843\n",
      "Epoch 155: train loss = 0.0002755636 valid loss = 0.0003143309\n",
      "Epoch 156: train loss = 0.0002755228 valid loss = 0.0003142844\n",
      "Epoch 157: train loss = 0.0002754882 valid loss = 0.0003142454\n",
      "Epoch 158: train loss = 0.0002754603 valid loss = 0.0003142125\n",
      "Epoch 159: train loss = 0.0002754380 valid loss = 0.0003141837\n",
      "Epoch 160: train loss = 0.0002754217 valid loss = 0.0003141618\n",
      "Epoch 161: train loss = 0.0002754081 valid loss = 0.0003141431\n",
      "Epoch 162: train loss = 0.0002753915 valid loss = 0.0003141241\n",
      "Epoch 163: train loss = 0.0002753716 valid loss = 0.0003141025\n",
      "Epoch 164: train loss = 0.0002753523 valid loss = 0.0003140820\n",
      "Epoch 165: train loss = 0.0002753285 valid loss = 0.0003140591\n",
      "Epoch 166: train loss = 0.0002753047 valid loss = 0.0003140355\n",
      "Epoch 167: train loss = 0.0002752828 valid loss = 0.0003140124\n",
      "Epoch 168: train loss = 0.0002752577 valid loss = 0.0003139869\n",
      "Epoch 169: train loss = 0.0002752326 valid loss = 0.0003139600\n",
      "Epoch 170: train loss = 0.0002752062 valid loss = 0.0003139327\n",
      "Epoch 171: train loss = 0.0002751753 valid loss = 0.0003139030\n",
      "Epoch 172: train loss = 0.0002751405 valid loss = 0.0003138713\n",
      "Epoch 173: train loss = 0.0002751058 valid loss = 0.0003138399\n",
      "Epoch 174: train loss = 0.0002750778 valid loss = 0.0003138123\n",
      "Epoch 175: train loss = 0.0002750503 valid loss = 0.0003137849\n",
      "Epoch 176: train loss = 0.0002750220 valid loss = 0.0003137574\n",
      "Epoch 177: train loss = 0.0002749934 valid loss = 0.0003137296\n",
      "Epoch 178: train loss = 0.0002749649 valid loss = 0.0003137021\n",
      "Epoch 179: train loss = 0.0002749365 valid loss = 0.0003136756\n",
      "Epoch 180: train loss = 0.0002749068 valid loss = 0.0003136494\n",
      "Epoch 181: train loss = 0.0002748783 valid loss = 0.0003136240\n",
      "Epoch 182: train loss = 0.0002748513 valid loss = 0.0003136016\n",
      "Epoch 183: train loss = 0.0002748260 valid loss = 0.0003135820\n",
      "Epoch 184: train loss = 0.0002748049 valid loss = 0.0003135673\n",
      "Epoch 185: train loss = 0.0002747858 valid loss = 0.0003135550\n",
      "Epoch 186: train loss = 0.0002747686 valid loss = 0.0003135436\n",
      "Epoch 187: train loss = 0.0002747539 valid loss = 0.0003135342\n",
      "Epoch 188: train loss = 0.0002747435 valid loss = 0.0003135309\n",
      "Epoch 189: train loss = 0.0002747364 valid loss = 0.0003135314\n",
      "Epoch 190: train loss = 0.0002747315 valid loss = 0.0003135338\n",
      "Epoch 191: train loss = 0.0002747301 valid loss = 0.0003135398\n",
      "Epoch 192: train loss = 0.0002747252 valid loss = 0.0003135403\n",
      "Epoch 193: train loss = 0.0002747212 valid loss = 0.0003135410\n",
      "Epoch 194: train loss = 0.0002747222 valid loss = 0.0003135472\n",
      "Epoch 195: train loss = 0.0002747333 valid loss = 0.0003135647\n",
      "Epoch 196: train loss = 0.0002747497 valid loss = 0.0003135880\n",
      "Epoch 197: train loss = 0.0002747443 valid loss = 0.0003135861\n",
      "Epoch 198: train loss = 0.0002747154 valid loss = 0.0003135569\n",
      "Epoch 199: train loss = 0.0002746737 valid loss = 0.0003135129\n",
      "Epoch 200: train loss = 0.0002746266 valid loss = 0.0003134623\n",
      "Epoch 201: train loss = 0.0002745798 valid loss = 0.0003134119\n",
      "Epoch 202: train loss = 0.0002745262 valid loss = 0.0003133530\n",
      "Epoch 203: train loss = 0.0002744678 valid loss = 0.0003132862\n",
      "Epoch 204: train loss = 0.0002744135 valid loss = 0.0003132220\n",
      "Epoch 205: train loss = 0.0002743620 valid loss = 0.0003131575\n",
      "Epoch 206: train loss = 0.0002743239 valid loss = 0.0003131074\n",
      "Epoch 207: train loss = 0.0002742963 valid loss = 0.0003130695\n",
      "Epoch 208: train loss = 0.0002742759 valid loss = 0.0003130408\n",
      "Epoch 209: train loss = 0.0002742636 valid loss = 0.0003130179\n",
      "Epoch 210: train loss = 0.0002742555 valid loss = 0.0003130017\n",
      "Epoch 211: train loss = 0.0002742535 valid loss = 0.0003129902\n",
      "Epoch 212: train loss = 0.0002742693 valid loss = 0.0003129946\n",
      "Epoch 213: train loss = 0.0002742809 valid loss = 0.0003129985\n",
      "Epoch 214: train loss = 0.0002742915 valid loss = 0.0003130037\n",
      "Epoch 215: train loss = 0.0002742941 valid loss = 0.0003130029\n",
      "Epoch 216: train loss = 0.0002742987 valid loss = 0.0003130034\n",
      "Epoch 217: train loss = 0.0002742892 valid loss = 0.0003129929\n",
      "Epoch 218: train loss = 0.0002742915 valid loss = 0.0003129928\n",
      "Epoch 219: train loss = 0.0002742825 valid loss = 0.0003129828\n",
      "Epoch 220: train loss = 0.0002742535 valid loss = 0.0003129561\n",
      "Epoch 221: train loss = 0.0002742312 valid loss = 0.0003129342\n",
      "Epoch 222: train loss = 0.0002742292 valid loss = 0.0003129290\n",
      "Epoch 223: train loss = 0.0002742185 valid loss = 0.0003129164\n",
      "Epoch 224: train loss = 0.0002741990 valid loss = 0.0003128959\n",
      "Epoch 225: train loss = 0.0002741652 valid loss = 0.0003128631\n",
      "Epoch 226: train loss = 0.0002741260 valid loss = 0.0003128261\n",
      "Epoch 227: train loss = 0.0002741053 valid loss = 0.0003128038\n",
      "Epoch 228: train loss = 0.0002740573 valid loss = 0.0003127586\n",
      "Epoch 229: train loss = 0.0002740078 valid loss = 0.0003127123\n",
      "Epoch 230: train loss = 0.0002739513 valid loss = 0.0003126606\n",
      "Epoch 231: train loss = 0.0002738913 valid loss = 0.0003126063\n",
      "Epoch 232: train loss = 0.0002738318 valid loss = 0.0003125530\n",
      "Epoch 233: train loss = 0.0002737782 valid loss = 0.0003125054\n",
      "Epoch 234: train loss = 0.0002737274 valid loss = 0.0003124610\n",
      "Epoch 235: train loss = 0.0002736855 valid loss = 0.0003124234\n",
      "Epoch 236: train loss = 0.0002736412 valid loss = 0.0003123847\n",
      "Epoch 237: train loss = 0.0002735916 valid loss = 0.0003123426\n",
      "Epoch 238: train loss = 0.0002735525 valid loss = 0.0003123086\n",
      "Epoch 239: train loss = 0.0002735141 valid loss = 0.0003122756\n",
      "Epoch 240: train loss = 0.0002734730 valid loss = 0.0003122432\n",
      "Epoch 241: train loss = 0.0002734360 valid loss = 0.0003122161\n",
      "Epoch 242: train loss = 0.0002734059 valid loss = 0.0003121911\n",
      "Epoch 243: train loss = 0.0002733778 valid loss = 0.0003121661\n",
      "Epoch 244: train loss = 0.0002733517 valid loss = 0.0003121436\n",
      "Epoch 245: train loss = 0.0002733268 valid loss = 0.0003121234\n",
      "Epoch 246: train loss = 0.0002733024 valid loss = 0.0003121024\n",
      "Epoch 247: train loss = 0.0002732808 valid loss = 0.0003120852\n",
      "Epoch 248: train loss = 0.0002732619 valid loss = 0.0003120714\n",
      "Epoch 249: train loss = 0.0002732446 valid loss = 0.0003120586\n",
      "Epoch 250: train loss = 0.0002732315 valid loss = 0.0003120511\n",
      "Epoch 251: train loss = 0.0002732197 valid loss = 0.0003120438\n",
      "Epoch 252: train loss = 0.0002732138 valid loss = 0.0003120438\n",
      "Epoch 253: train loss = 0.0002732094 valid loss = 0.0003120445\n",
      "Epoch 254: train loss = 0.0002732114 valid loss = 0.0003120529\n",
      "Epoch 255: train loss = 0.0002732029 valid loss = 0.0003120466\n",
      "Epoch 256: train loss = 0.0002731926 valid loss = 0.0003120386\n",
      "Epoch 257: train loss = 0.0002731870 valid loss = 0.0003120366\n",
      "Epoch 258: train loss = 0.0002731633 valid loss = 0.0003120115\n",
      "Epoch 259: train loss = 0.0002731311 valid loss = 0.0003119759\n",
      "Epoch 260: train loss = 0.0002730929 valid loss = 0.0003119321\n",
      "Epoch 261: train loss = 0.0002730549 valid loss = 0.0003118874\n",
      "Epoch 262: train loss = 0.0002730253 valid loss = 0.0003118524\n",
      "Epoch 263: train loss = 0.0002729996 valid loss = 0.0003118221\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 264: train loss = 0.0002729759 valid loss = 0.0003117943\n",
      "Epoch 265: train loss = 0.0002729510 valid loss = 0.0003117633\n",
      "Epoch 266: train loss = 0.0002729290 valid loss = 0.0003117365\n",
      "Epoch 267: train loss = 0.0002729087 valid loss = 0.0003117103\n",
      "Epoch 268: train loss = 0.0002728910 valid loss = 0.0003116878\n",
      "Epoch 269: train loss = 0.0002728754 valid loss = 0.0003116661\n",
      "Epoch 270: train loss = 0.0002728616 valid loss = 0.0003116447\n",
      "Epoch 271: train loss = 0.0002728501 valid loss = 0.0003116262\n",
      "Epoch 272: train loss = 0.0002728409 valid loss = 0.0003116112\n",
      "Epoch 273: train loss = 0.0002728297 valid loss = 0.0003115963\n",
      "Epoch 274: train loss = 0.0002728170 valid loss = 0.0003115816\n",
      "Epoch 275: train loss = 0.0002728009 valid loss = 0.0003115655\n",
      "Epoch 276: train loss = 0.0002727825 valid loss = 0.0003115490\n",
      "Epoch 277: train loss = 0.0002727679 valid loss = 0.0003115338\n",
      "Epoch 278: train loss = 0.0002727569 valid loss = 0.0003115196\n",
      "Epoch 279: train loss = 0.0002727455 valid loss = 0.0003115050\n",
      "Epoch 280: train loss = 0.0002727335 valid loss = 0.0003114914\n",
      "Epoch 281: train loss = 0.0002727246 valid loss = 0.0003114789\n",
      "Epoch 282: train loss = 0.0002727111 valid loss = 0.0003114646\n",
      "Epoch 283: train loss = 0.0002726974 valid loss = 0.0003114503\n",
      "Epoch 284: train loss = 0.0002726806 valid loss = 0.0003114340\n",
      "Epoch 285: train loss = 0.0002726637 valid loss = 0.0003114178\n",
      "Epoch 286: train loss = 0.0002726455 valid loss = 0.0003114016\n",
      "Epoch 287: train loss = 0.0002726251 valid loss = 0.0003113865\n",
      "Epoch 288: train loss = 0.0002726069 valid loss = 0.0003113714\n",
      "Epoch 289: train loss = 0.0002725911 valid loss = 0.0003113596\n",
      "Epoch 290: train loss = 0.0002725753 valid loss = 0.0003113464\n",
      "Epoch 291: train loss = 0.0002725601 valid loss = 0.0003113331\n",
      "Epoch 292: train loss = 0.0002725449 valid loss = 0.0003113183\n",
      "Epoch 293: train loss = 0.0002725311 valid loss = 0.0003113061\n",
      "Epoch 294: train loss = 0.0002725180 valid loss = 0.0003112943\n",
      "Epoch 295: train loss = 0.0002725083 valid loss = 0.0003112886\n",
      "Epoch 296: train loss = 0.0002725002 valid loss = 0.0003112850\n",
      "Epoch 297: train loss = 0.0002724932 valid loss = 0.0003112829\n",
      "Epoch 298: train loss = 0.0002724938 valid loss = 0.0003112911\n",
      "Epoch 299: train loss = 0.0002724986 valid loss = 0.0003113045\n",
      "Epoch 300: train loss = 0.0002725117 valid loss = 0.0003113275\n",
      "Epoch 301: train loss = 0.0002725196 valid loss = 0.0003113434\n",
      "Epoch 302: train loss = 0.0002725184 valid loss = 0.0003113470\n",
      "Epoch 303: train loss = 0.0002725069 valid loss = 0.0003113372\n",
      "Epoch 304: train loss = 0.0002724904 valid loss = 0.0003113211\n",
      "Epoch 305: train loss = 0.0002724752 valid loss = 0.0003113068\n",
      "Epoch 306: train loss = 0.0002724599 valid loss = 0.0003112931\n",
      "Epoch 307: train loss = 0.0002724389 valid loss = 0.0003112723\n",
      "Epoch 308: train loss = 0.0002724165 valid loss = 0.0003112496\n",
      "Epoch 309: train loss = 0.0002723980 valid loss = 0.0003112314\n",
      "Epoch 310: train loss = 0.0002723775 valid loss = 0.0003112108\n",
      "Epoch 311: train loss = 0.0002723526 valid loss = 0.0003111843\n",
      "Epoch 312: train loss = 0.0002723262 valid loss = 0.0003111539\n",
      "Epoch 313: train loss = 0.0002722948 valid loss = 0.0003111155\n",
      "Epoch 314: train loss = 0.0002722697 valid loss = 0.0003110837\n",
      "Epoch 315: train loss = 0.0002722475 valid loss = 0.0003110517\n",
      "Epoch 316: train loss = 0.0002722345 valid loss = 0.0003110304\n",
      "Epoch 317: train loss = 0.0002722255 valid loss = 0.0003110163\n",
      "Epoch 318: train loss = 0.0002722161 valid loss = 0.0003110036\n",
      "Epoch 319: train loss = 0.0002722035 valid loss = 0.0003109899\n",
      "Epoch 320: train loss = 0.0002721867 valid loss = 0.0003109757\n",
      "Epoch 321: train loss = 0.0002721678 valid loss = 0.0003109618\n",
      "Epoch 322: train loss = 0.0002721523 valid loss = 0.0003109477\n",
      "Epoch 323: train loss = 0.0002721355 valid loss = 0.0003109343\n",
      "Epoch 324: train loss = 0.0002721201 valid loss = 0.0003109239\n",
      "Epoch 325: train loss = 0.0002721073 valid loss = 0.0003109194\n",
      "Epoch 326: train loss = 0.0002720973 valid loss = 0.0003109140\n",
      "Epoch 327: train loss = 0.0002720892 valid loss = 0.0003109105\n",
      "Epoch 328: train loss = 0.0002720845 valid loss = 0.0003109114\n",
      "Epoch 329: train loss = 0.0002720852 valid loss = 0.0003109192\n",
      "Epoch 330: train loss = 0.0002720883 valid loss = 0.0003109289\n",
      "Epoch 331: train loss = 0.0002720911 valid loss = 0.0003109369\n",
      "Epoch 332: train loss = 0.0002720917 valid loss = 0.0003109411\n",
      "Epoch 333: train loss = 0.0002720772 valid loss = 0.0003109250\n",
      "Epoch 334: train loss = 0.0002720603 valid loss = 0.0003109054\n",
      "Epoch 335: train loss = 0.0002720532 valid loss = 0.0003108987\n",
      "Epoch 336: train loss = 0.0002720508 valid loss = 0.0003108987\n",
      "Epoch 337: train loss = 0.0002720519 valid loss = 0.0003109036\n",
      "Epoch 338: train loss = 0.0002720466 valid loss = 0.0003109003\n",
      "Epoch 339: train loss = 0.0002720459 valid loss = 0.0003109023\n",
      "Epoch 340: train loss = 0.0002720336 valid loss = 0.0003108889\n",
      "Epoch 341: train loss = 0.0002720276 valid loss = 0.0003108830\n",
      "Epoch 342: train loss = 0.0002720129 valid loss = 0.0003108660\n",
      "Epoch 343: train loss = 0.0002719886 valid loss = 0.0003108379\n",
      "Epoch 344: train loss = 0.0002719577 valid loss = 0.0003108014\n",
      "Epoch 345: train loss = 0.0002719327 valid loss = 0.0003107729\n",
      "Epoch 346: train loss = 0.0002719026 valid loss = 0.0003107375\n",
      "Epoch 347: train loss = 0.0002718787 valid loss = 0.0003107097\n",
      "Epoch 348: train loss = 0.0002718528 valid loss = 0.0003106786\n",
      "Epoch 349: train loss = 0.0002718334 valid loss = 0.0003106565\n",
      "Epoch 350: train loss = 0.0002718077 valid loss = 0.0003106267\n",
      "Epoch 351: train loss = 0.0002717909 valid loss = 0.0003106100\n",
      "Epoch 352: train loss = 0.0002717765 valid loss = 0.0003105976\n",
      "Epoch 353: train loss = 0.0002717664 valid loss = 0.0003105915\n",
      "Epoch 354: train loss = 0.0002717586 valid loss = 0.0003105879\n",
      "Epoch 355: train loss = 0.0002717560 valid loss = 0.0003105906\n",
      "Epoch 356: train loss = 0.0002717475 valid loss = 0.0003105848\n",
      "Epoch 357: train loss = 0.0002717465 valid loss = 0.0003105884\n",
      "Epoch 358: train loss = 0.0002717445 valid loss = 0.0003105900\n",
      "Epoch 359: train loss = 0.0002717486 valid loss = 0.0003105983\n",
      "Epoch 360: train loss = 0.0002717568 valid loss = 0.0003106108\n",
      "Epoch 361: train loss = 0.0002717647 valid loss = 0.0003106224\n",
      "Epoch 362: train loss = 0.0002717686 valid loss = 0.0003106287\n",
      "Epoch 363: train loss = 0.0002717759 valid loss = 0.0003106385\n",
      "Epoch 364: train loss = 0.0002717827 valid loss = 0.0003106478\n",
      "Epoch 365: train loss = 0.0002717918 valid loss = 0.0003106600\n",
      "Epoch 366: train loss = 0.0002717893 valid loss = 0.0003106581\n",
      "Epoch 367: train loss = 0.0002717734 valid loss = 0.0003106414\n",
      "Epoch 368: train loss = 0.0002717488 valid loss = 0.0003106149\n",
      "Epoch 369: train loss = 0.0002717340 valid loss = 0.0003105997\n",
      "Epoch 370: train loss = 0.0002717255 valid loss = 0.0003105914\n",
      "Epoch 371: train loss = 0.0002717262 valid loss = 0.0003105939\n",
      "Epoch 372: train loss = 0.0002717112 valid loss = 0.0003105777\n",
      "Epoch 373: train loss = 0.0002717000 valid loss = 0.0003105654\n",
      "Epoch 374: train loss = 0.0002717063 valid loss = 0.0003105733\n",
      "Epoch 375: train loss = 0.0002717312 valid loss = 0.0003106021\n",
      "Epoch 376: train loss = 0.0002717375 valid loss = 0.0003106097\n",
      "Epoch 377: train loss = 0.0002717335 valid loss = 0.0003106063\n",
      "Epoch 378: train loss = 0.0002717388 valid loss = 0.0003106140\n",
      "Epoch 379: train loss = 0.0002717135 valid loss = 0.0003105863\n",
      "Epoch 380: train loss = 0.0002716713 valid loss = 0.0003105394\n",
      "Epoch 381: train loss = 0.0002716351 valid loss = 0.0003104990\n",
      "Epoch 382: train loss = 0.0002715923 valid loss = 0.0003104512\n",
      "Epoch 383: train loss = 0.0002715420 valid loss = 0.0003103938\n",
      "Epoch 384: train loss = 0.0002714852 valid loss = 0.0003103295\n",
      "Epoch 385: train loss = 0.0002714406 valid loss = 0.0003102788\n",
      "Epoch 386: train loss = 0.0002713826 valid loss = 0.0003102109\n",
      "Epoch 387: train loss = 0.0002713411 valid loss = 0.0003101617\n",
      "Epoch 388: train loss = 0.0002713071 valid loss = 0.0003101214\n",
      "Epoch 389: train loss = 0.0002712754 valid loss = 0.0003100826\n",
      "Epoch 390: train loss = 0.0002712464 valid loss = 0.0003100459\n",
      "Epoch 391: train loss = 0.0002712264 valid loss = 0.0003100208\n",
      "Epoch 392: train loss = 0.0002712046 valid loss = 0.0003099890\n",
      "Epoch 393: train loss = 0.0002711917 valid loss = 0.0003099648\n",
      "Epoch 394: train loss = 0.0002711855 valid loss = 0.0003099508\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 395: train loss = 0.0002711793 valid loss = 0.0003099399\n",
      "Epoch 396: train loss = 0.0002711730 valid loss = 0.0003099297\n",
      "Epoch 397: train loss = 0.0002711694 valid loss = 0.0003099215\n",
      "Epoch 398: train loss = 0.0002711622 valid loss = 0.0003099115\n",
      "Epoch 399: train loss = 0.0002711469 valid loss = 0.0003098970\n",
      "Epoch 400: train loss = 0.0002711323 valid loss = 0.0003098826\n",
      "Epoch 401: train loss = 0.0002711194 valid loss = 0.0003098689\n",
      "Epoch 402: train loss = 0.0002711117 valid loss = 0.0003098582\n",
      "Epoch 403: train loss = 0.0002710988 valid loss = 0.0003098434\n",
      "Epoch 404: train loss = 0.0002710824 valid loss = 0.0003098263\n",
      "Epoch 405: train loss = 0.0002710636 valid loss = 0.0003098081\n",
      "Epoch 406: train loss = 0.0002710448 valid loss = 0.0003097914\n",
      "Epoch 407: train loss = 0.0002710183 valid loss = 0.0003097707\n",
      "Epoch 408: train loss = 0.0002710375 valid loss = 0.0003097781\n",
      "Epoch 409: train loss = 0.0002710527 valid loss = 0.0003097846\n",
      "Epoch 410: train loss = 0.0002710591 valid loss = 0.0003097854\n",
      "Epoch 411: train loss = 0.0002710653 valid loss = 0.0003097863\n",
      "Epoch 412: train loss = 0.0002710601 valid loss = 0.0003097788\n",
      "Epoch 413: train loss = 0.0002710536 valid loss = 0.0003097702\n",
      "Epoch 414: train loss = 0.0002710569 valid loss = 0.0003097686\n",
      "Epoch 415: train loss = 0.0002710480 valid loss = 0.0003097583\n",
      "Epoch 416: train loss = 0.0002710326 valid loss = 0.0003097436\n",
      "Epoch 417: train loss = 0.0002710113 valid loss = 0.0003097253\n",
      "Epoch 418: train loss = 0.0002709924 valid loss = 0.0003097091\n",
      "Epoch 419: train loss = 0.0002709810 valid loss = 0.0003096968\n",
      "Epoch 420: train loss = 0.0002709633 valid loss = 0.0003096810\n",
      "Epoch 421: train loss = 0.0002709473 valid loss = 0.0003096674\n",
      "Epoch 422: train loss = 0.0002709303 valid loss = 0.0003096547\n",
      "Epoch 423: train loss = 0.0002709172 valid loss = 0.0003096436\n",
      "Epoch 424: train loss = 0.0002709069 valid loss = 0.0003096324\n",
      "Epoch 425: train loss = 0.0002708991 valid loss = 0.0003096221\n",
      "Epoch 426: train loss = 0.0002708884 valid loss = 0.0003096110\n",
      "Epoch 427: train loss = 0.0002708767 valid loss = 0.0003095993\n",
      "Epoch 428: train loss = 0.0002708667 valid loss = 0.0003095884\n",
      "Epoch 429: train loss = 0.0002708526 valid loss = 0.0003095772\n",
      "Epoch 430: train loss = 0.0002708376 valid loss = 0.0003095660\n",
      "Epoch 431: train loss = 0.0002708213 valid loss = 0.0003095559\n",
      "Epoch 432: train loss = 0.0002708081 valid loss = 0.0003095492\n",
      "Epoch 433: train loss = 0.0002707961 valid loss = 0.0003095408\n",
      "Epoch 434: train loss = 0.0002707856 valid loss = 0.0003095342\n",
      "Epoch 435: train loss = 0.0002707769 valid loss = 0.0003095291\n",
      "Epoch 436: train loss = 0.0002707667 valid loss = 0.0003095208\n",
      "Epoch 437: train loss = 0.0002707592 valid loss = 0.0003095179\n",
      "Epoch 438: train loss = 0.0002707520 valid loss = 0.0003095147\n",
      "Epoch 439: train loss = 0.0002707477 valid loss = 0.0003095154\n",
      "Epoch 440: train loss = 0.0002707400 valid loss = 0.0003095108\n",
      "Epoch 441: train loss = 0.0002707444 valid loss = 0.0003095229\n",
      "Epoch 442: train loss = 0.0002707455 valid loss = 0.0003095293\n",
      "Epoch 443: train loss = 0.0002707366 valid loss = 0.0003095218\n",
      "Epoch 444: train loss = 0.0002707313 valid loss = 0.0003095185\n",
      "Epoch 445: train loss = 0.0002707085 valid loss = 0.0003094928\n",
      "Epoch 446: train loss = 0.0002706864 valid loss = 0.0003094667\n",
      "Epoch 447: train loss = 0.0002706718 valid loss = 0.0003094510\n",
      "Epoch 448: train loss = 0.0002706419 valid loss = 0.0003094123\n",
      "Epoch 449: train loss = 0.0002706178 valid loss = 0.0003093800\n",
      "Epoch 450: train loss = 0.0002706017 valid loss = 0.0003093580\n",
      "Epoch 451: train loss = 0.0002705892 valid loss = 0.0003093399\n",
      "Epoch 452: train loss = 0.0002705808 valid loss = 0.0003093255\n",
      "Epoch 453: train loss = 0.0002705717 valid loss = 0.0003093156\n",
      "Epoch 454: train loss = 0.0002705627 valid loss = 0.0003093069\n",
      "Epoch 455: train loss = 0.0002705522 valid loss = 0.0003092984\n",
      "Epoch 456: train loss = 0.0002705439 valid loss = 0.0003092922\n",
      "Epoch 457: train loss = 0.0002705351 valid loss = 0.0003092871\n",
      "Epoch 458: train loss = 0.0002705262 valid loss = 0.0003092826\n",
      "Epoch 459: train loss = 0.0002705193 valid loss = 0.0003092833\n",
      "Epoch 460: train loss = 0.0002705105 valid loss = 0.0003092767\n",
      "Epoch 461: train loss = 0.0002705041 valid loss = 0.0003092751\n",
      "Epoch 462: train loss = 0.0002705008 valid loss = 0.0003092766\n",
      "Epoch 463: train loss = 0.0002704975 valid loss = 0.0003092771\n",
      "Epoch 464: train loss = 0.0002705012 valid loss = 0.0003092868\n",
      "Epoch 465: train loss = 0.0002705103 valid loss = 0.0003093026\n",
      "Epoch 466: train loss = 0.0002705186 valid loss = 0.0003093159\n",
      "Epoch 467: train loss = 0.0002705028 valid loss = 0.0003092964\n",
      "Epoch 468: train loss = 0.0002704861 valid loss = 0.0003092747\n",
      "Epoch 469: train loss = 0.0002704761 valid loss = 0.0003092621\n",
      "Epoch 470: train loss = 0.0002704732 valid loss = 0.0003092588\n",
      "Epoch 471: train loss = 0.0002704629 valid loss = 0.0003092461\n",
      "Epoch 472: train loss = 0.0002704475 valid loss = 0.0003092260\n",
      "Epoch 473: train loss = 0.0002704249 valid loss = 0.0003091951\n",
      "Epoch 474: train loss = 0.0002704079 valid loss = 0.0003091715\n",
      "Epoch 475: train loss = 0.0002703879 valid loss = 0.0003091408\n",
      "Epoch 476: train loss = 0.0002703752 valid loss = 0.0003091221\n",
      "Epoch 477: train loss = 0.0002703649 valid loss = 0.0003091058\n",
      "Epoch 478: train loss = 0.0002703565 valid loss = 0.0003090892\n",
      "Epoch 479: train loss = 0.0002703506 valid loss = 0.0003090750\n",
      "Epoch 480: train loss = 0.0002703425 valid loss = 0.0003090630\n",
      "Epoch 481: train loss = 0.0002703316 valid loss = 0.0003090508\n",
      "Epoch 482: train loss = 0.0002703222 valid loss = 0.0003090401\n",
      "Epoch 483: train loss = 0.0002703133 valid loss = 0.0003090300\n",
      "Epoch 484: train loss = 0.0002703081 valid loss = 0.0003090199\n",
      "Epoch 485: train loss = 0.0002703056 valid loss = 0.0003090117\n",
      "Epoch 486: train loss = 0.0002702994 valid loss = 0.0003090021\n",
      "Epoch 487: train loss = 0.0002702882 valid loss = 0.0003089902\n",
      "Epoch 488: train loss = 0.0002702786 valid loss = 0.0003089797\n",
      "Epoch 489: train loss = 0.0002702640 valid loss = 0.0003089669\n",
      "Epoch 490: train loss = 0.0002702473 valid loss = 0.0003089530\n",
      "Epoch 491: train loss = 0.0002702344 valid loss = 0.0003089406\n",
      "Epoch 492: train loss = 0.0002702235 valid loss = 0.0003089284\n",
      "Epoch 493: train loss = 0.0002702111 valid loss = 0.0003089182\n",
      "Epoch 494: train loss = 0.0002702007 valid loss = 0.0003089090\n",
      "Epoch 495: train loss = 0.0002701914 valid loss = 0.0003088997\n",
      "Epoch 496: train loss = 0.0002701834 valid loss = 0.0003088945\n",
      "Epoch 497: train loss = 0.0002701775 valid loss = 0.0003088933\n",
      "Epoch 498: train loss = 0.0002701715 valid loss = 0.0003088893\n",
      "Epoch 499: train loss = 0.0002701645 valid loss = 0.0003088845\n",
      "Epoch 500: train loss = 0.0002701556 valid loss = 0.0003088761\n"
     ]
    }
   ],
   "source": [
    "training_loss_list = []\n",
    "valid_loss_list = []\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "  \n",
    "    for iteration in range(n_epochs):\n",
    "        x_batch, y_batch = get_next_batch(batch_size)\n",
    "        sess.run(optimizer, feed_dict={input_X: x_batch, input_y:y_batch})\n",
    "        \n",
    "        train_loss = loss.eval(feed_dict={input_X: X_train, input_y: y_train})\n",
    "        valid_loss = loss.eval(feed_dict={input_X: X_val, input_y: y_val})\n",
    "        \n",
    "        training_loss_list.append(train_loss)\n",
    "        valid_loss_list.append(valid_loss)\n",
    "        \n",
    "        print('Epoch {0}: train loss = {1:.10f} valid loss = {2:.10f}'.format(iteration + 1, train_loss, valid_loss))\n",
    "\n",
    "    y_test_pred = sess.run(outputs, feed_dict={input_X: X_test})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8ldK-ijMgB87"
   },
   "source": [
    "with our model trained its time to plot the loss for each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "colab_type": "code",
    "id": "t7lwXj5HgB89",
    "outputId": "bac330a6-1b2d-4ff7-b967-72816368d2c9",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(12,4))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(training_loss_list, color='blue', label='train loss')\n",
    "plt.plot(valid_loss_list, color='red', label='validation loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.legend()\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(y_test_pred[0:100, -1], color='blue', label='Predicted') #its much data will printing just 100 entries\n",
    "plt.plot(y_test[0:100, -1], color='red', label='True')\n",
    "plt.ylabel('WITHDRAW AMT')\n",
    "plt.xlabel('Entries')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BWFWRDBvgB9C"
   },
   "source": [
    "like you can see our loss is decreasing over time it is good notices for us"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vVor4r3fzF4Y"
   },
   "source": [
    "the predictions has been done too, so the next step is mount our data again\n",
    "to visualize our result and answer the questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B6KEz2KLgB9I"
   },
   "outputs": [],
   "source": [
    "X_test = X_test.reshape(-1, 5)\n",
    "results = pd.DataFrame({'Date': X_test[:, 0],\n",
    "                        'VALUE DATE': X_test[:, 1],\n",
    "                        'DEPOSIT AMT': X_test[:, 2],\n",
    "                        'BALANCE AMT': X_test[:, 3],\n",
    "                        'client_class': X_test[:, 4],\n",
    "                        'WITHDRAWAL AMT': y_test_pred.reshape(-1,)\n",
    "                       })\n",
    "\n",
    "most_withdrawal_accounts_value = results.sort_values(by='WITHDRAWAL AMT', ascending=False).iloc[0:9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Lee1783jgB9N"
   },
   "source": [
    "like previosly above, we transform all `Account No` in a interger value and store it in a column called `client_class` we see that the most evaluated client or with most potential client is the client with `client_class` column with value 0 but why that client is considered with most potential?\n",
    "\n",
    "we presume that column `WITHDRAWAL AMT` its not so about a CASH WITHDRAWAL, like column `TRANSACTION DETAILS` show to us, the dataset have a resume about all transfers in the bank this means all payment, deposit etc... in resume: the client with label 0 is taking money from your account and investing in another account he is our 'BIG FISH' but, what account is the column `client_class` with value 0? its we find in follow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "QiAe8mHDgB9P",
    "outputId": "e9092c3e-4f5e-4d2f-90f1-21af36db9cfa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "409000493201'\n"
     ]
    }
   ],
   "source": [
    "client_class = most_withdrawal_accounts_value.iloc[0]['client_class'] ## GET CLASS OF OUR BIG FISH\n",
    "\n",
    "for idx in range(len(accounts_label)):\n",
    "    if(idx == client_class):\n",
    "        print(accounts_label[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "32cjcVh3gB9U"
   },
   "source": [
    "Our BIG FISH is the client with `Account No` = `409000611074'`. \n",
    "its time to approve to her a new line of credit and give a limitless platinum card. \\o/ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HmAbV4wzgB9V"
   },
   "source": [
    "### THANKS ###"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "lstm_gru_with_tensorflow_eduardo_asafe.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
